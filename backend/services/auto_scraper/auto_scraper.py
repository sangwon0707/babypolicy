"""
ë²”ìš© PDF ìë™ ìŠ¤í¬ë˜í¼ ver1.4.1 - ë©”ì¸ í”„ë¡œê·¸ë¨

ver1.4.1 í•µì‹¬ ê°œì„ ì‚¬í•­:
ğŸ”’ ê°œì¸ì •ë³´ ë³´í˜¸ (ì •ë³´í†µì‹ ë§ë²• ì¤€ìˆ˜):
    - ì´ë©”ì¼ ì£¼ì†Œ ìë™ ìˆ˜ì§‘ ê¸ˆì§€
    - ì „í™”ë²ˆí˜¸, ì£¼ë¯¼ë²ˆí˜¸ ë“± ê°œì¸ì •ë³´ í•„í„°ë§
    - PDF íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ (ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ì°¨ë‹¨)
    - ë¡œê·¸ì— ê°œì¸ì •ë³´ ìë™ ì œê±°

ğŸ¯ íƒ­/ë„¤ë¹„ê²Œì´ì…˜ í•„í„°ë§:
    - íƒ­ ë©”ë‰´(ê³µê³ ë¬¸, ì„ëŒ€ê°€ì´ë“œ ë“±) ìë™ í•„í„°ë§
    - í˜ì´ì§•/ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œ ì œì™¸
    - ê²Œì‹œê¸€ í’ˆì§ˆ ê²€ì¦ ê°•í™” (URL íŒ¨í„´, í…ìŠ¤íŠ¸ ê¸¸ì´, í…Œì´ë¸” êµ¬ì¡°)
    - í•„í„°ë§ í†µê³„ ë¡œê¹… ì¶”ê°€

ver1.4 í•µì‹¬ ê°œì„ ì‚¬í•­:
ğŸ¯ ì•ˆì •ì„± ê°•í™”:
    - ê²Œì‹œíŒ URL ì¶”ì¶œ ë¡œì§ ê°œì„  (ê°„ë‹¨í•˜ê³  ì•ˆì •ì )
    - ì¤‘ë³µ ì œê±° ë¡œì§ ìˆ˜ì • (URL + í…ìŠ¤íŠ¸ í•´ì‹œ ì¡°í•©)
    - ì—°ì† ì—ëŸ¬ ë³µêµ¬ ê°•í™” (5íšŒ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ì í™•ì¸)
    - íƒ€ì„ì•„ì›ƒ ìµœì í™” (ë¶ˆí•„ìš”í•œ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•)

âš¡ ì„±ëŠ¥ ê°œì„ :
    - í˜ì´ì§€ ë‹¨ìœ„ í•™ìŠµ ë¡œì§ (ì²« ê²Œì‹œê¸€ì—ì„œ í•™ìŠµ â†’ ë‚˜ë¨¸ì§€ ë¹ ë¥¸ ëª¨ë“œ)
    - ë¹ ë¥¸ ëª¨ë“œ: í•™ìŠµëœ ì „ëµë§Œ ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ ì „ì²´ íƒì§€ë¡œ í´ë°±
    - ì‹¤ì‹œê°„ ì§„í–‰ ë¡œê·¸ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)

ver1.3 ê¸°ëŠ¥ ìœ ì§€:
    - ì „ëµ íŒ¨í„´ í•™ìŠµ ì‹œìŠ¤í…œ
    - ê²Œì‹œíŒ URLë³„ ì „ëµ ì„±ê³µë¥  ì¶”ì 
    - ì´ë¯¸ì§€ ì°¨ë‹¨ ë©”ëª¨ë¦¬ ì ˆì•½

ì‚¬ìš©ë²•:
    python auto_scraper.py
"""

import os
import sys
import io
import time
from datetime import datetime
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì • (ì´ëª¨ì§€ ì¶œë ¥ ì§€ì›)
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from config import *
from utils import *
from pdf_detector import (
    auto_detect_and_download_all,
    auto_detect_and_download_fast,  # ver1.4: ë¹ ë¥¸ ëª¨ë“œ
    analyze_page_structure,
    save_learned_strategies,  # ver1.3: í•™ìŠµ ë°ì´í„° ì €ì¥
    reset_learned_strategies,  # ver1.3.1: í•™ìŠµ ë°ì´í„° ì´ˆê¸°í™”
    get_learned_strategies_stats  # ver1.3.1: í•™ìŠµ í†µê³„ ì¡°íšŒ
)

# ver1.2: DB ì‚¬ìš©
if USE_DATABASE:
    try:
        from database import (
            get_download_stats, migrate_from_json,
            is_article_processed, mark_article_processed,
            cleanup_old_processed_articles, save_board_pattern,
            get_board_pattern
        )
    except ImportError:
        debug_log("database.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 'WARNING')
        USE_DATABASE = False


# ============================================
# ê²Œì‹œíŒ ë§í¬ ì¶”ì¶œ (ver1.1 ì•ˆì •ì  ë°©ì‹)
# ============================================

def is_navigation_or_tab_element(element):
    """
    ver1.4.1: íƒ­ ë©”ë‰´ë‚˜ ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œ í•„í„°ë§

    Args:
        element: WebElement ê°ì²´

    Returns:
        bool: Trueë©´ íƒ­/ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œ (ì œì™¸), Falseë©´ ê²Œì‹œê¸€ ë§í¬
    """
    try:
        # 1. í´ë˜ìŠ¤ëª… ì²´í¬
        class_name = element.get_attribute('class') or ''
        class_lower = class_name.lower()

        # íƒ­/ë„¤ë¹„ê²Œì´ì…˜ í´ë˜ìŠ¤ íŒ¨í„´
        nav_class_patterns = [
            'tab', 'nav', 'menu', 'header', 'gnb', 'lnb',
            'breadcrumb', 'pagination', 'paging', 'pageindexer'
        ]

        if any(pattern in class_lower for pattern in nav_class_patterns):
            debug_log(f"íƒ­/ë„¤ë¹„ê²Œì´ì…˜ ì œì™¸ (í´ë˜ìŠ¤): {class_name}", 'VERBOSE')
            return True

        # 2. í…ìŠ¤íŠ¸ íŒ¨í„´ ì²´í¬
        text = element.text.strip()

        # íƒ­ ë©”ë‰´ í…ìŠ¤íŠ¸ íŒ¨í„´ (í•œê¸€/ì˜ë¬¸)
        tab_text_patterns = [
            'ê³µê³ ë¬¸', 'ì„ëŒ€ê°€ì´ë“œ', 'ì²­ì•½ì—°ìŠµ', 'ê°€ì´ë“œ', 'ì—°ìŠµ',
            'tab', 'guide', 'practice', 'manual', 'tutorial',
            'ì´ì „', 'ë‹¤ìŒ', 'ì²˜ìŒ', 'ë§ˆì§€ë§‰',  # í˜ì´ì§•
            'prev', 'next', 'first', 'last'
        ]

        text_lower = text.lower()
        if any(pattern in text_lower for pattern in tab_text_patterns):
            debug_log(f"íƒ­ ë©”ë‰´ ì œì™¸ (í…ìŠ¤íŠ¸): {text}", 'VERBOSE')
            return True

        # 3. ë¶€ëª¨ ìš”ì†Œ ì²´í¬ (íƒ­ ì»¨í…Œì´ë„ˆ ì•ˆì— ìˆëŠ”ì§€)
        try:
            parent = element.find_element(By.XPATH, '..')
            parent_class = (parent.get_attribute('class') or '').lower()

            if any(pattern in parent_class for pattern in nav_class_patterns):
                debug_log(f"íƒ­ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ìš”ì†Œ ì œì™¸: {text}", 'VERBOSE')
                return True
        except:
            pass

        # 4. ìœ„ì¹˜ ê¸°ë°˜ í•„í„° (ìƒë‹¨ ê³ ì • ì˜ì—­)
        try:
            # Y ì¢Œí‘œê°€ 200px ì´í•˜ì´ê³  í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ìƒë‹¨ ë©”ë‰´ì¼ ê°€ëŠ¥ì„±
            location = element.location
            if location['y'] < 200 and len(text) < 15:
                # ì¶”ê°€ ê²€ì¦: hrefê°€ ê²Œì‹œíŒ íŒ¨í„´ì´ ì•„ë‹ˆë©´ íƒ­ìœ¼ë¡œ ê°„ì£¼
                href = element.get_attribute('href') or ''
                is_board_pattern = any(pattern in href.lower() for pattern in BOARD_LINK_PATTERNS)

                if not is_board_pattern:
                    debug_log(f"ìƒë‹¨ ì˜ì—­ ì œì™¸: {text} (y={location['y']})", 'VERBOSE')
                    return True
        except:
            pass

        return False

    except Exception as e:
        debug_log(f"íƒ­ í•„í„°ë§ ì˜¤ë¥˜", 'WARNING', e)
        return False  # ì˜¤ë¥˜ ì‹œ ê²Œì‹œê¸€ë¡œ ê°„ì£¼


def is_valid_article_link(element, text):
    """
    ver1.4.1: ê²Œì‹œê¸€ ë§í¬ í’ˆì§ˆ ê²€ì¦
    ver1.4.2: LH ì‚¬ì´íŠ¸ ëŒ€ì‘ ì¶”ê°€

    Args:
        element: WebElement ê°ì²´
        text: ë§í¬ í…ìŠ¤íŠ¸

    Returns:
        bool: Trueë©´ ìœ íš¨í•œ ê²Œì‹œê¸€ ë§í¬
    """
    try:
        # ğŸ†• LH ì‚¬ì´íŠ¸ ì „ìš©: .wrtancInfoBtn í´ë˜ìŠ¤ëŠ” ë¬´ì¡°ê±´ í†µê³¼
        element_class = (element.get_attribute('class') or '').lower()
        if 'wrtancinfobtn' in element_class:
            debug_log(f"LH ê²Œì‹œê¸€ ë§í¬ ê°ì§€: {text[:50]}", 'VERBOSE')
            return True

        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ìœ¼ë©´ íƒ­ëª…ì¼ ê°€ëŠ¥ì„±)
        if len(text) < 8:  # ìµœì†Œ 8ì ì´ìƒ
            debug_log(f"í…ìŠ¤íŠ¸ ë„ˆë¬´ ì§§ìŒ ì œì™¸: {text}", 'VERBOSE')
            return False

        # 2. URL íŒ¨í„´ ê²€ì¦
        href = element.get_attribute('href') or ''

        # ê²Œì‹œê¸€ ID íŒŒë¼ë¯¸í„° ì²´í¬ (board_id, no, idx, seq ë“±)
        article_id_patterns = ['no=', 'idx=', 'seq=', 'id=', 'num=', 'board_id=', 'article']
        has_article_id = any(pattern in href.lower() for pattern in article_id_patterns)

        # onclickë„ ì²´í¬
        onclick = element.get_attribute('onclick') or ''
        has_onclick_article = any(pattern in onclick.lower() for pattern in ['view', 'detail', 'show'])

        if not (has_article_id or has_onclick_article):
            # hrefì™€ onclick ë‘˜ ë‹¤ ê²Œì‹œê¸€ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì˜ì‹¬
            if href and href != '#':
                debug_log(f"ê²Œì‹œê¸€ íŒ¨í„´ ì—†ìŒ ì œì™¸: {text[:30]}", 'VERBOSE')
                return False

        # 3. ê²Œì‹œê¸€ íŠ¹ì„± ìš”ì†Œ í™•ì¸ (ë‚ ì§œ, ì¡°íšŒìˆ˜ ë“±ì´ í¬í•¨ëœ í–‰)
        try:
            # ë¶€ëª¨ <tr>ì—ì„œ <td> ê°œìˆ˜ í™•ì¸
            parent_tr = element.find_element(By.XPATH, './ancestor::tr[1]')
            tds = parent_tr.find_elements(By.TAG_NAME, 'td')

            # ì¼ë°˜ì ìœ¼ë¡œ ê²Œì‹œíŒì€ 3ê°œ ì´ìƒì˜ td (ë²ˆí˜¸, ì œëª©, ë‚ ì§œ ë“±)
            if len(tds) >= 3:
                debug_log(f"ê²Œì‹œíŒ í–‰ êµ¬ì¡° í™•ì¸: {text[:30]} (td={len(tds)})", 'VERBOSE')
                return True
        except:
            pass

        # 4. ëª¨ë“  ê²€ì¦ í†µê³¼
        return True

    except Exception as e:
        debug_log(f"ê²Œì‹œê¸€ ê²€ì¦ ì˜¤ë¥˜", 'WARNING', e)
        return True  # ì˜¤ë¥˜ ì‹œ ê²Œì‹œê¸€ë¡œ ê°„ì£¼


def extract_board_links(driver):
    """
    ê²Œì‹œíŒ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ìë™ ì¶”ì¶œ
    ver1.4.1: íƒ­/ë„¤ë¹„ê²Œì´ì…˜ í•„í„°ë§ ê°•í™”

    Args:
        driver: WebDriver ê°ì²´

    Returns:
        list: [{"url": "...", "title": "...", "element": WebElement}, ...]
    """
    debug_log("ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ ì‹œì‘ (ver1.4.1 - íƒ­ í•„í„°ë§ ê°•í™”)", 'INFO')

    # âœ… 1. <a> íƒœê·¸ ê²€ìƒ‰
    all_links = driver.find_elements(By.TAG_NAME, "a")
    debug_log(f"<a> íƒœê·¸ ìˆ˜: {len(all_links)}", 'DEBUG')

    # âœ… 2. onclick ì†ì„±ì´ ìˆëŠ” ëª¨ë“  ìš”ì†Œ ê²€ìƒ‰ (tr, div, span ë“±)
    all_onclick_elements = driver.find_elements(By.XPATH, "//*[@onclick]")
    debug_log(f"onclick ìš”ì†Œ ìˆ˜: {len(all_onclick_elements)}", 'DEBUG')

    # í•©ì¹˜ê¸°
    all_elements = all_links + all_onclick_elements
    debug_log(f"ì´ ê²€ì‚¬ ëŒ€ìƒ: {len(all_elements)}ê°œ", 'DEBUG')

    article_links = []
    filtered_count = {'tab': 0, 'invalid': 0, 'safe': 0, 'text_length': 0, 'exclude_pattern': 0}

    for element in all_elements:
        try:
            href = element.get_attribute('href')
            onclick = element.get_attribute('onclick')
            tag_name = element.tag_name.lower()

            # âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìš”ì†Œ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
            if tag_name == 'tr':
                # <tr> íƒœê·¸ì¸ ê²½ìš°: tdë“¤ì—ì„œ ì œëª© ì°¾ê¸°
                try:
                    tds = element.find_elements(By.TAG_NAME, 'td')
                    if len(tds) >= 2:
                        text = tds[1].text.strip()
                    else:
                        text = element.text.strip()
                except:
                    text = element.text.strip()
            else:
                text = element.text.strip()

            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if not text:
                continue

            # 1. ì•ˆì „ì„± ê²€ì¦
            if not is_safe_element(element):
                filtered_count['safe'] += 1
                continue

            # 2. ver1.4.1: íƒ­/ë„¤ë¹„ê²Œì´ì…˜ í•„í„°ë§
            if is_navigation_or_tab_element(element):
                filtered_count['tab'] += 1
                continue

            # 3. ver1.4.1: ê°œì¸ì •ë³´ ë³´í˜¸ - ì´ë©”ì¼ ì£¼ì†Œ í¬í•¨ ì—¬ë¶€ ì²´í¬
            if ENABLE_PRIVACY_PROTECTION and BLOCK_EMAIL_COLLECTION:
                has_personal, info_types = contains_personal_info(text)
                if has_personal:
                    debug_log(f"ğŸ”’ ê°œì¸ì •ë³´ í¬í•¨ ë§í¬ ì œì™¸: {sanitize_for_logging(text)}", 'WARNING')
                    filtered_count['exclude_pattern'] += 1
                    continue

            # 4. í…ìŠ¤íŠ¸ ê¸¸ì´ í•„í„°
            if len(text) < MIN_LINK_TEXT_LENGTH or len(text) > MAX_LINK_TEXT_LENGTH:
                filtered_count['text_length'] += 1
                continue

            # 5. ì œì™¸ íŒ¨í„´ í™•ì¸
            if any(exclude_text in text.lower() for exclude_text in EXCLUDE_LINK_TEXTS):
                filtered_count['exclude_pattern'] += 1
                continue

            # 6. ver1.4.1: ê²Œì‹œê¸€ ë§í¬ í’ˆì§ˆ ê²€ì¦
            if not is_valid_article_link(element, text):
                filtered_count['invalid'] += 1
                continue

            # ğŸ†• LH ì‚¬ì´íŠ¸ ì „ìš©: .wrtancInfoBtn í´ë˜ìŠ¤ ì²˜ë¦¬
            element_class = (element.get_attribute('class') or '').lower()
            if 'wrtancinfobtn' in element_class:
                article_links.append({
                    'url': None,
                    'title': text,
                    'element': element,
                    'is_javascript': True
                })
                debug_log(f"LH JavaScript ë§í¬ ì¶”ê°€: {text[:50]}", 'VERBOSE')
                continue

            # âœ… 6. JavaScript ë§í¬ ì²´í¬ (onclick ì†ì„±)
            if onclick:
                onclick_lower = onclick.lower()
                js_patterns = ['view', 'detail', 'show', 'read', 'open', 'go', 'move']

                if any(pattern in onclick_lower for pattern in js_patterns):
                    article_links.append({
                        'url': None,
                        'title': text,
                        'element': element,
                        'is_javascript': True
                    })
                    debug_log(f"JavaScript ë§í¬ ë°œê²¬ ({tag_name}): {text[:50]} (onclick={onclick[:50]})", 'VERBOSE')
                    continue

            # 7. ì¼ë°˜ URL ë§í¬ ì²´í¬
            if href and href != '#':
                href_lower = href.lower()
                is_board_link = any(pattern in href_lower for pattern in BOARD_LINK_PATTERNS)

                if is_board_link:
                    article_links.append({
                        'url': href,
                        'title': text,
                        'element': element,
                        'is_javascript': False
                    })
                    debug_log(f"ì¼ë°˜ ë§í¬ ë°œê²¬: {text[:50]}", 'VERBOSE')

        except Exception as e:
            debug_log(f"ë§í¬ ë¶„ì„ ì˜¤ë¥˜", 'WARNING', e)
            continue

    # ì¤‘ë³µ ì œê±°
    unique_links = []
    seen_identifiers = set()

    for link in article_links:
        if link.get('is_javascript'):
            identifier = link['title']
        else:
            identifier = link['url']

        if identifier not in seen_identifiers:
            unique_links.append(link)
            seen_identifiers.add(identifier)

    # í†µê³„ ì¶œë ¥
    js_count = sum(1 for link in unique_links if link.get('is_javascript', False))
    url_count = len(unique_links) - js_count

    debug_log(f"ğŸš« í•„í„°ë§ í†µê³„: íƒ­={filtered_count['tab']}, í’ˆì§ˆë¶ˆëŸ‰={filtered_count['invalid']}, "
              f"ì•ˆì „ì„±={filtered_count['safe']}, ê¸¸ì´={filtered_count['text_length']}, "
              f"ì œì™¸íŒ¨í„´={filtered_count['exclude_pattern']}", 'INFO')
    debug_log(f"âœ… ì´ {len(unique_links)}ê°œ ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ ì™„ë£Œ (JavaScript: {js_count}ê°œ, ì¼ë°˜: {url_count}ê°œ)", 'INFO')

    return unique_links


# ============================================
# ê²Œì‹œê¸€ ì²˜ë¦¬
# ============================================

def process_article(driver, wait, link_info, download_dir):
    """
    ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ (PDF íƒì§€ ë° ë‹¤ìš´ë¡œë“œ)

    Args:
        driver: WebDriver ê°ì²´
        wait: WebDriverWait ê°ì²´
        link_info: ê²Œì‹œê¸€ ì •ë³´ dict
        download_dir: ë‹¤ìš´ë¡œë“œ í´ë”

    Returns:
        dict: {"status": "success/no_pdf/error", "downloads": [...], ...}
    """
    try:
        article_title = link_info.get('title', 'ì œëª©ì—†ìŒ')
        is_javascript = link_info.get('is_javascript', False)
        article_url = link_info.get('url')

        # âœ… JavaScript ë§í¬ ì²˜ë¦¬
        if is_javascript:
            element = link_info['element']
            debug_log(f"JavaScript ë§í¬ í´ë¦­: {article_title[:50]}", 'DEBUG')

            try:
                element.click()
                debug_log("ì¼ë°˜ í´ë¦­ ì„±ê³µ", 'DEBUG')
            except Exception as click_error:
                debug_log(f"ì¼ë°˜ í´ë¦­ ì‹¤íŒ¨, JavaScript í´ë¦­ ì‹œë„", 'DEBUG', click_error)
                driver.execute_script("arguments[0].click();", element)
                debug_log("JavaScript í´ë¦­ ì„±ê³µ", 'DEBUG')

            wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])
            article_url = driver.current_url

        else:
            # âœ… ì¼ë°˜ URL ë§í¬ ì²˜ë¦¬
            if not article_url:
                raise Exception("ê²Œì‹œê¸€ URLì´ ì—†ìŠµë‹ˆë‹¤")

            debug_log(f"ê²Œì‹œê¸€ URL ì´ë™: {article_url}", 'DEBUG')
            driver.get(article_url)
            wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])

        # ë‹¤ì¤‘ PDF íƒì§€ ë° ë‹¤ìš´ë¡œë“œ
        result = auto_detect_and_download_all(
            driver=driver,
            wait=wait,
            download_dir=download_dir,
            article_title=article_title,
            article_url=article_url
        )

        # ê²°ê³¼ì— ê²Œì‹œê¸€ ì •ë³´ ì¶”ê°€
        result['article_title'] = article_title
        result['article_url'] = article_url

        return result

    except Exception as e:
        debug_log(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì˜¤ë¥˜", 'ERROR', e)
        return {
            'status': 'error',
            'message': str(e),
            'article_title': link_info.get('title', 'ì œëª©ì—†ìŒ'),
            'article_url': link_info.get('url', 'N/A'),
            'downloads': [],
            'skipped': []
        }


# ============================================
# í˜ì´ì§€ë„¤ì´ì…˜
# ============================================

def find_next_page_button(driver, current_page):
    """ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸°"""
    debug_log(f"ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ê²€ìƒ‰ ì¤‘ (í˜„ì¬: {current_page})", 'DEBUG')

    try:
        next_page = current_page + 1

        # ë°©ë²• 1: í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ ì°¾ê¸° (1, 2, 3, ...)
        page_number_selectors = [
            f"//a[text()='{next_page}']",
            f"//a[contains(@href, 'page={next_page}')]",
            f"//a[@data-page='{next_page}']",
            f"//button[text()='{next_page}']",
            f"//div[text()='{next_page}' and (@onclick or @data-page)]"
        ]

        for selector in page_number_selectors:
            try:
                element = driver.find_element(By.XPATH, selector)
                if element.is_displayed() and element.is_enabled():
                    debug_log(f"âœ… í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ ë°œê²¬: {next_page}", 'DEBUG')
                    return {
                        'available': True,
                        'element': element,
                        'method': 'page_number',
                        'page': next_page
                    }
            except NoSuchElementException:
                continue

        # ë°©ë²• 2: "ë‹¤ìŒ", "Next", ">" ë²„íŠ¼ ì°¾ê¸°
        next_button_keywords = []
        for lang_keywords in NEXT_PAGE_KEYWORDS.values():
            next_button_keywords.extend(lang_keywords)
        next_button_keywords.extend(['>', 'â€º', 'Â»', 'next'])

        for keyword in next_button_keywords:
            try:
                elements = driver.find_elements(By.XPATH,
                    f"//a[contains(text(), '{keyword}')] | "
                    f"//button[contains(text(), '{keyword}')] | "
                    f"//span[contains(text(), '{keyword}')]/parent::a | "
                    f"//span[contains(text(), '{keyword}')]/parent::button"
                )

                for element in elements:
                    class_name = element.get_attribute('class') or ''
                    if 'disabled' in class_name.lower() or 'inactive' in class_name.lower():
                        continue

                    if element.is_displayed() and element.is_enabled():
                        debug_log(f"âœ… ë‹¤ìŒ ë²„íŠ¼ ë°œê²¬: '{keyword}'", 'DEBUG')
                        return {
                            'available': True,
                            'element': element,
                            'method': 'next_button',
                            'page': next_page
                        }

            except:
                continue

        # ë§ˆì§€ë§‰ í˜ì´ì§€
        debug_log("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë§ˆì§€ë§‰ í˜ì´ì§€)", 'INFO')
        return {
            'available': False,
            'element': None,
            'method': None,
            'page': current_page
        }

    except Exception as e:
        debug_log("ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ê²€ìƒ‰ ì˜¤ë¥˜", 'ERROR', e)
        return {
            'available': False,
            'element': None,
            'method': None,
            'page': current_page
        }


def navigate_to_next_page(driver, current_page):
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
    try:
        next_info = find_next_page_button(driver, current_page)

        if not next_info['available']:
            return {'success': False, 'page': current_page}

        element = next_info['element']

        try:
            element.click()
        except:
            driver.execute_script("arguments[0].click();", element)

        debug_log(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘... (ë°©ë²•: {next_info['method']})", 'INFO')

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(2)
        wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])

        debug_log(f"âœ… í˜ì´ì§€ {next_info['page']} ì´ë™ ì™„ë£Œ", 'INFO')

        return {'success': True, 'page': next_info['page']}

    except Exception as e:
        debug_log("í˜ì´ì§€ ì´ë™ ì˜¤ë¥˜", 'ERROR', e)
        return {'success': False, 'page': current_page}


# ============================================
# í†µê³„ ì¶œë ¥ ë° ë¡œê¹… (ver1.4 ê°œì„ )
# ============================================

def log_progress(page, article_idx, total, status):
    """ê°„ê²°í•œ ì§„í–‰ ë¡œê·¸ (ver1.4)"""
    from datetime import datetime
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] P{page} {article_idx}/{total} | {status}")


def display_page_summary(page, success_count, skipped_count, no_pdf_count, error_count):
    """í˜ì´ì§€ë³„ í†µê³„ ì¶œë ¥"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š í˜ì´ì§€ {page} ì™„ë£Œ")
    print(f"   âœ… ì„±ê³µ: {success_count}ê°œ | â­ï¸ ê±´ë„ˆëœ€: {skipped_count}ê°œ | âš ï¸ PDFì—†ìŒ: {no_pdf_count}ê°œ | âŒ ì‹¤íŒ¨: {error_count}ê°œ")

    # ì „ì²´ ì§„í–‰ ìƒí™©
    download_log = load_download_log()
    total_downloaded = download_log['total_downloaded']
    print(f"ğŸ’¾ ì „ì²´ ì§„í–‰: {total_downloaded}ê°œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    print(f"{'='*60}\n")


def display_final_summary(start_time, total_pages, success_count, skipped_count, no_pdf_count, error_count):
    """ìµœì¢… í†µê³„ ì¶œë ¥"""
    elapsed_time = time.time() - start_time

    print(f"\n{'='*60}")
    print(f"ğŸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ğŸ“„ ì²˜ë¦¬í•œ í˜ì´ì§€: {total_pages}ê°œ")
    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {success_count}ê°œ")
    print(f"â­ï¸ ê±´ë„ˆëœ€: {skipped_count}ê°œ (ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨)")
    print(f"âš ï¸ PDF ì—†ìŒ: {no_pdf_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {format_time(elapsed_time)}")
    print(f"{'='*60}\n")


# ============================================
# ë©”ì¸ ìŠ¤í¬ë˜í¼
# ============================================

def run_scraper(board_url, download_dir, resume_from=None):
    """
    ë©”ì¸ ìŠ¤í¬ë˜í¼ ì‹¤í–‰
    """
    # ë“œë¼ì´ë²„ ì„¤ì •
    driver = setup_driver(download_dir)
    wait = WebDriverWait(driver, TIMEOUT['element_wait'])

    # ì‹œì‘ ì •ë³´
    if resume_from:
        current_page = resume_from['current_page']
        resume_article_index = resume_from['current_article_index'] + 1
        board_url = resume_from.get('board_url', board_url)
        print(f"ğŸ”„ í˜ì´ì§€ {current_page}, {resume_article_index + 1}ë²ˆì§¸ ê²Œì‹œê¸€ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.\n")
    else:
        current_page = 1
        resume_article_index = 0

    # í†µê³„
    start_time = time.time()
    total_pages_processed = 0
    total_success = 0
    total_skipped = 0
    total_no_pdf = 0
    total_error = 0

    processed_count = 0

    try:
        while True:  # Ctrl+Cê¹Œì§€ ë¬´í•œ ì‹¤í–‰
            try:
                # í˜„ì¬ í˜ì´ì§€ URL ìƒì„±
                if '?' in board_url:
                    current_url = f"{board_url}&page={current_page}"
                else:
                    # í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì´ë¯¸ URLì— ìˆì„ ìˆ˜ ìˆìŒ
                    current_url = board_url

                print(f"\n{'='*60}")
                print(f"ğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
                print(f"{'='*60}")
                print(f"ğŸ”— URL: {current_url}\n")

                # í˜ì´ì§€ ì ‘ì†
                driver.get(current_url)
                wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])

                # ê²Œì‹œê¸€ ê°œìˆ˜ íŒŒì•… (ì²« ì¶”ì¶œ)
                article_links = extract_board_links(driver)

                if not article_links:
                    print("âš ï¸ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("í˜ì´ì§€ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...\n")
                    analyze_page_structure(driver)

                    response = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/n]: ")
                    if response.lower() != 'y':
                        break

                    nav_result = navigate_to_next_page(driver, current_page)
                    if nav_result['success']:
                        current_page = nav_result['page']
                        continue
                    else:
                        print("ë‹¤ìŒ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break

                total_articles = len(article_links)
                print(f"ğŸ“ ê²Œì‹œê¸€ {total_articles}ê°œ ë°œê²¬\n")

                # í˜ì´ì§€ë³„ í†µê³„
                page_success = 0
                page_skipped = 0
                page_no_pdf = 0
                page_error = 0

                # ver1.4: ì—°ì† ì—ëŸ¬ ì¹´ìš´í„°
                MAX_CONSECUTIVE_ERRORS = 5
                error_count = 0

                # ver1.4: í˜ì´ì§€ë³„ í•™ìŠµ ì „ëµ (ì´ˆê¸°í™”)
                page_learned_strategy = None

                # ê° ê²Œì‹œê¸€ ì²˜ë¦¬ (ì¸ë±ìŠ¤ ê¸°ë°˜)
                for idx in range(total_articles):
                    # ì¬ì‹œì‘ ì‹œ ê±´ë„ˆë›°ê¸°
                    if current_page == (resume_from or {}).get('current_page', 0) and idx < resume_article_index:
                        print(f"  â­ï¸ [{idx + 1}/{total_articles}] ê±´ë„ˆëœ€ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
                        continue

                    # âœ… ë§¤ë²ˆ ê²Œì‹œíŒ ë§í¬ ë‹¤ì‹œ ì¶”ì¶œ (stale element ë°©ì§€)
                    article_links = extract_board_links(driver)

                    if idx >= len(article_links):
                        debug_log(f"ê²Œì‹œê¸€ {idx + 1}ë²ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì´ {len(article_links)}ê°œ)", 'WARNING')
                        break

                    link_info = article_links[idx]
                    article_title = link_info['title']

                    # ver1.4: ì§„í–‰ ë¡œê·¸ ì¶œë ¥
                    log_progress(current_page, idx + 1, total_articles, f"ì²˜ë¦¬ ì¤‘: {article_title[:30]}")

                    # ver1.4: í˜ì´ì§€ ë‹¨ìœ„ í•™ìŠµ ë¡œì§
                    try:
                        if idx == 0:
                            # ì²« ê²Œì‹œê¸€: ì „ì²´ íƒì§€ + í•™ìŠµ
                            print(f"  [{idx + 1}/{total_articles}] í•™ìŠµ ëª¨ë“œ (ëª¨ë“  ì „ëµ ì‹œë„)")
                            result = process_article(driver, wait, link_info, download_dir)

                            # ì„±ê³µí•œ ì „ëµ ì €ì¥
                            if result['status'] in ['success', 'partial']:
                                page_learned_strategy = result.get('successful_strategy')
                                if page_learned_strategy:
                                    print(f"    âœ“ í•™ìŠµ ì™„ë£Œ: {page_learned_strategy}")
                        else:
                            # ë‚˜ë¨¸ì§€: ë¹ ë¥¸ ëª¨ë“œ
                            if page_learned_strategy:
                                print(f"  [{idx + 1}/{total_articles}] ë¹ ë¥¸ ëª¨ë“œ ({page_learned_strategy})")
                                # ë¹ ë¥¸ ëª¨ë“œë¡œ ì²˜ë¦¬ - process_article ëŒ€ì‹  ì§ì ‘ í˜¸ì¶œ
                                article_url_temp = driver.current_url
                                result_fast = auto_detect_and_download_fast(
                                    driver, wait, download_dir,
                                    page_learned_strategy,
                                    article_title, article_url_temp
                                )
                                result = {
                                    **result_fast,
                                    'article_title': article_title,
                                    'article_url': article_url_temp
                                }
                            else:
                                # í•™ìŠµ ì‹¤íŒ¨ ì‹œ ì „ì²´ íƒì§€
                                print(f"  [{idx + 1}/{total_articles}] í•™ìŠµ ì—†ìŒ (ì „ì²´ íƒì§€)")
                                result = process_article(driver, wait, link_info, download_dir)

                        error_count = 0  # ì„±ê³µ ì‹œ ë¦¬ì…‹
                    except Exception as e:
                        error_count += 1
                        debug_log(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", 'ERROR', e)

                        if error_count >= MAX_CONSECUTIVE_ERRORS:
                            print(f"\nâš ï¸ ì—°ì† {MAX_CONSECUTIVE_ERRORS}íšŒ ì‹¤íŒ¨. ê²Œì‹œíŒ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±")
                            user_response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/n]: ")
                            if user_response.lower() != 'y':
                                print("ì‚¬ìš©ìê°€ ì¤‘ë‹¨ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                                break
                            error_count = 0  # ê³„ì†í•˜ë©´ ë¦¬ì…‹

                        # ì—ëŸ¬ ê²°ê³¼ ìƒì„±
                        result = {
                            'status': 'error',
                            'message': str(e),
                            'article_title': article_title,
                            'article_url': 'N/A',
                            'downloads': [],
                            'skipped': []
                        }

                    # ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    article_url = result.get('article_url', 'N/A')

                    # ë‹¤ì¤‘ PDF ê²°ê³¼ ì²˜ë¦¬
                    downloads = result.get('downloads', [])
                    skipped = result.get('skipped', [])

                    if result['status'] in ['success', 'partial']:
                        # ë‹¤ìš´ë¡œë“œ ì„±ê³µ
                        for dl in downloads:
                            filename = dl.get('filename', 'unknown')
                            size = dl.get('size', 'unknown')
                            print(f"    âœ… ì„±ê³µ: {filename} ({size})")

                            add_download_record(
                                title=article_title,
                                filename=filename,
                                url=article_url,
                                size=size,
                                method=dl.get('method', 'unknown')
                            )

                        page_success += len(downloads)
                        total_success += len(downloads)

                        # ê±´ë„ˆë›´ íŒŒì¼ë“¤
                        page_skipped += len(skipped)
                        total_skipped += len(skipped)

                        if result['status'] == 'partial':
                            print(f"    âš ï¸ ì¼ë¶€ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

                    elif result['status'] == 'all_skipped':
                        # ëª¨ë‘ ê±´ë„ˆëœ€
                        for sk in skipped:
                            print(f"    â­ï¸ ê±´ë„ˆëœ€: {sk.get('filename', 'unknown')}")

                        page_skipped += len(skipped)
                        total_skipped += len(skipped)

                    elif result['status'] == 'no_pdf':
                        print(f"    âš ï¸ PDF ì—†ìŒ")
                        if USE_DATABASE:
                            from database import add_failure_record as db_add_failure
                            db_add_failure(article_title, article_url, 'no_pdf')
                        else:
                            add_failure_record(article_title, article_url, 'no_pdf')

                        page_no_pdf += 1
                        total_no_pdf += 1

                    else:  # error
                        message = result.get('message', 'unknown')
                        print(f"    âŒ ì‹¤íŒ¨: {message}")

                        if USE_DATABASE:
                            from database import add_failure_record as db_add_failure
                            db_add_failure(article_title, article_url, message)
                        else:
                            add_failure_record(article_title, article_url, message)

                        page_error += 1
                        total_error += 1

                    # Checkpoint ì €ì¥
                    download_log = load_download_log()
                    save_checkpoint(
                        page=current_page,
                        article_index=idx,
                        board_url=current_url,
                        total_downloaded=download_log['total_downloaded']
                    )

                    # âœ… ê²Œì‹œíŒ ëª©ë¡ìœ¼ë¡œ ë³µê·€ (ì•ˆì •ì„± ê°•í™”)
                    try:
                        debug_log(f"ê²Œì‹œíŒ ë³µê·€: {current_url}", 'DEBUG')
                        driver.get(current_url)
                        wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])
                        time.sleep(1)

                        # ê²Œì‹œíŒ í˜ì´ì§€ê°€ ë§ëŠ”ì§€ í™•ì¸
                        if "list" not in driver.current_url.lower() and "board" not in driver.current_url.lower():
                            debug_log("ê²Œì‹œíŒ ë³µê·€ í™•ì¸ ì‹¤íŒ¨, ì¬ì‹œë„", 'WARNING')
                            driver.get(current_url)
                            wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])

                    except Exception as e:
                        debug_log("ê²Œì‹œíŒ ë³µê·€ ì¤‘ ì˜¤ë¥˜, ì¬ì‹œë„", 'WARNING', e)
                        driver.get(current_url)
                        wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])
                        time.sleep(1)

                    # ë¸Œë¼ìš°ì € ì£¼ê¸°ì  ì¬ì‹œì‘
                    processed_count += 1
                    if processed_count % BROWSER_RESTART_INTERVAL == 0:
                        print(f"\n  ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘... ({processed_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ)")
                        driver = restart_browser(driver, download_dir)
                        wait = WebDriverWait(driver, TIMEOUT['element_wait'])
                        driver.get(current_url)
                        wait_for_page_stable(driver, timeout=TIMEOUT['page_load'])

                # í˜ì´ì§€ í†µê³„ ì¶œë ¥
                display_page_summary(current_page, page_success, page_skipped, page_no_pdf, page_error)
                total_pages_processed += 1

                # ver1.3: í˜ì´ì§€ ì¢…ë£Œ ì‹œ í•™ìŠµ ë°ì´í„° ì €ì¥ (ë©”ëª¨ë¦¬ â†’ JSON)
                if USE_STRATEGY_LEARNING:
                    save_learned_strategies()
                    debug_log(f"ğŸ“š í˜ì´ì§€ {current_page} ì¢…ë£Œ: í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ", 'DEBUG')

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                print(f"ğŸ“„ ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì¤‘...")
                nav_result = navigate_to_next_page(driver, current_page)

                if not nav_result['success']:
                    print(f"\n{'='*60}")
                    print(f"ğŸ‰ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬! ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    print(f"{'='*60}\n")
                    break

                current_page = nav_result['page']
                resume_article_index = 0  # ìƒˆ í˜ì´ì§€ëŠ” ì²˜ìŒë¶€í„°

            except KeyboardInterrupt:
                print(f"\n\n{'='*60}")
                print(f"ğŸ›‘ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤ (Ctrl+C)")
                print(f"{'='*60}\n")
                break

            except Exception as e:
                debug_log(f"í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", 'ERROR', e)
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ë¸Œë¼ìš°ì €ë¥¼ ì¬ì‹œì‘í•˜ê³  ê³„ì†í•©ë‹ˆë‹¤...\n")

                # ë¸Œë¼ìš°ì € ì¬ì‹œì‘
                driver = restart_browser(driver, download_dir)
                wait = WebDriverWait(driver, TIMEOUT['element_wait'])
                continue

    finally:
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        try:
            driver.quit()
        except:
            pass

        # ver1.3: í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìµœì¢… í•™ìŠµ ë°ì´í„° ì €ì¥
        if USE_STRATEGY_LEARNING:
            save_learned_strategies()
            print("ğŸ“š í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ\n")

        # ìµœì¢… í†µê³„
        display_final_summary(start_time, total_pages_processed, total_success, total_skipped, total_no_pdf, total_error)


# ============================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì í™•ì¸
    auto_mode = len(sys.argv) > 1
    auto_url = sys.argv[1] if auto_mode else None

    print(f"\n{'='*70}")
    print(f"ë²”ìš© PDF ìë™ ìŠ¤í¬ë˜í¼ ver1.4.1")
    print(f"{'='*70}")
    print(f"ê°œì¸ì •ë³´ ë³´í˜¸: ì •ë³´í†µì‹ ë§ë²• ì¤€ìˆ˜ (ì´ë©”ì¼/ì „í™”ë²ˆí˜¸ ìˆ˜ì§‘ ì°¨ë‹¨)")
    print(f"ver1.4.1: íƒ­/ë„¤ë¹„ê²Œì´ì…˜ í•„í„°ë§ ê°•í™”ë¡œ í•™ìŠµ ì •í™•ë„ í–¥ìƒ!")
    print(f"PDF ì „ìš©: PDF íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ (ë‹¤ë¥¸ í˜•ì‹ ìë™ ì°¨ë‹¨)")
    print(f"ë¡œê·¸ ë³´ì•ˆ: ê°œì¸ì •ë³´ ìë™ ë§ˆìŠ¤í‚¹")
    print(f"{'='*70}\n")

    # ver1.3.1: ë°ì´í„° ê´€ë¦¬ ì˜µì…˜
    data_stats = get_data_stats()

    if USE_STRATEGY_LEARNING:
        # í˜„ì¬ ë°ì´í„° í˜„í™© í‘œì‹œ
        print(f"ğŸ“Š í˜„ì¬ ë°ì´í„° í˜„í™©:")
        print(f"   ğŸ“š í•™ìŠµëœ ê²Œì‹œíŒ: {data_stats['learning_boards']}ê°œ")
        print(f"   ğŸ“„ ë‹¤ìš´ë¡œë“œëœ PDF: {data_stats['pdf_count']}ê°œ")
        print(f"   ğŸ“ ë¡œê·¸ í¬ê¸°: {data_stats['log_size']:.1f} KB")
        if data_stats['has_checkpoint']:
            print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: ìˆìŒ")
        print()

        if data_stats['learning_boards'] > 0 or data_stats['pdf_count'] > 0:
            # ì´ˆê¸°í™” ì˜µì…˜ ì„ íƒ
            if auto_mode:
                # ìë™ ëª¨ë“œ: ì´ˆê¸°í™” ì•ˆ í•¨
                reset_option = '3'
                print(f"ğŸ”„ ìë™ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©\n")
            else:
                print(f"ğŸ”„ ì´ˆê¸°í™” ì˜µì…˜:")
                print(f"   1. í•™ìŠµ ë°ì´í„°ë§Œ ì´ˆê¸°í™” (PDF íŒŒì¼ ìœ ì§€)")
                print(f"   2. ì™„ì „ ì´ˆê¸°í™” (í•™ìŠµ ë°ì´í„° + PDF + ë¡œê·¸ ëª¨ë‘ ì‚­ì œ)")
                print(f"   3. ì´ˆê¸°í™” ì•ˆ í•¨ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)")
                print()

                reset_option = input("ì„ íƒ [1/2/3]: ").strip()

            if reset_option == '1':
                # í•™ìŠµ ë°ì´í„°ë§Œ ì´ˆê¸°í™”
                if reset_learned_strategies():
                    print(f"\n   âœ… í•™ìŠµ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ!")
                    print(f"   ğŸ“„ PDF íŒŒì¼ {data_stats['pdf_count']}ê°œëŠ” ìœ ì§€ë©ë‹ˆë‹¤.")
                    print(f"   â„¹ï¸  ëª¨ë“  ê²Œì‹œíŒì„ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n")
                else:
                    print(f"\n   âŒ ì´ˆê¸°í™” ì‹¤íŒ¨\n")

            elif reset_option == '2':
                # ì™„ì „ ì´ˆê¸°í™”
                confirm = input("âš ï¸  PDF íŒŒì¼ê³¼ ëª¨ë“  ë¡œê·¸ê°€ ì‚­ì œë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/n]: ")
                if confirm.lower() == 'y':
                    reset_stats = reset_all_data(keep_downloads=False)
                    print(f"\n   âœ… ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ!")
                    print(f"   ğŸ“š í•™ìŠµ ë°ì´í„°: {'ì‚­ì œë¨' if reset_stats['learning_data'] else 'ì—†ìŒ'}")
                    print(f"   ğŸ“„ PDF íŒŒì¼: {reset_stats['pdf_files']}ê°œ ì‚­ì œ")
                    print(f"   ğŸ“ ë¡œê·¸ íŒŒì¼: {reset_stats['log_files']}ê°œ ì‚­ì œ")
                    print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {'ì‚­ì œë¨' if reset_stats['checkpoint'] else 'ì—†ìŒ'}")
                    print(f"   ğŸ—„ï¸  ë°ì´í„°ë² ì´ìŠ¤: {'ì‚­ì œë¨' if reset_stats['database'] else 'ì—†ìŒ'}")
                    print(f"   â„¹ï¸  ì™„ì „íˆ ì²˜ìŒ ìƒíƒœë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.\n")
                else:
                    print(f"\n   â„¹ï¸  ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")

            else:
                # ì´ˆê¸°í™” ì•ˆ í•¨
                print(f"\n   â„¹ï¸  ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
        else:
            print(f"ğŸ“š ë°ì´í„° ì—†ìŒ (ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤)\n")

    # ver1.2: JSON â†’ DB ë§ˆì´ê·¸ë ˆì´ì…˜
    if USE_DATABASE:
        json_log = DOWNLOAD_LOG_FILE
        if os.path.exists(json_log):
            print(f"ğŸ’¾ ê¸°ì¡´ JSON ë¡œê·¸ë¥¼ DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
            count = migrate_from_json(json_log)
            if count > 0:
                print(f"   âœ… {count}ê°œ ë ˆì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ\n")
            else:
                print(f"   â„¹ï¸  ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\n")

    # Checkpoint í™•ì¸
    checkpoint = load_checkpoint()

    if checkpoint and not auto_mode:
        print(f"ğŸ’¾ ì´ì „ ì‘ì—…ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
        print(f"   í˜ì´ì§€: {checkpoint['current_page']}")
        print(f"   ì§„í–‰: {checkpoint['current_article_index'] + 1}ë²ˆì§¸ ê²Œì‹œê¸€")
        print(f"   URL: {checkpoint['board_url']}")
        print(f"   ë§ˆì§€ë§‰ ì‹¤í–‰: {checkpoint['last_updated']}")
        print(f"   ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {checkpoint['total_downloaded']}ê°œ\n")

        resume = input("ğŸ”„ ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/n]: ")

        if resume.lower() == 'y':
            board_url = checkpoint['board_url']
            download_dir = DOWNLOAD_DIR

            print(f"\nâœ… ì¬ê°œí•©ë‹ˆë‹¤.\n")
            print(f"{'='*60}\n")

            run_scraper(board_url, download_dir, resume_from=checkpoint)
            return

        else:
            print("\nìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\n")
            clear_checkpoint()

    # ìƒˆë¡œ ì‹œì‘
    if auto_mode:
        # ìë™ ëª¨ë“œ: ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ URL ë°›ìŒ
        board_url = auto_url
        download_dir = DOWNLOAD_DIR
        print(f"ğŸ“ URL: {board_url}")
        print(f"ğŸ“‚ í´ë”: {download_dir}\n")
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        board_url = input("ğŸ“ ê²Œì‹œíŒ URLì„ ì…ë ¥í•˜ì„¸ìš”:\n   ì˜ˆ) https://example.com/board\n\n> ")

        if not board_url.strip():
            print("âŒ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        download_dir_input = input(f"\nğŸ“‚ ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: {DOWNLOAD_DIR}):\n   ì—”í„°í‚¤ë¥¼ ëˆ„ë¥´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©\n\n> ")

        download_dir = download_dir_input.strip() if download_dir_input.strip() else DOWNLOAD_DIR

        print(f"\n{'='*60}")
        print(f"âœ… ì„¤ì • ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"ğŸ“ URL: {board_url}")
        print(f"ğŸ“‚ í´ë”: {download_dir}")
        print(f"{'='*60}\n")

        input("ì—”í„°í‚¤ë¥¼ ëˆ„ë¥´ë©´ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # í´ë” ìƒì„±
    os.makedirs(download_dir, exist_ok=True)

    print(f"\nğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘...\n")

    # ìŠ¤í¬ë˜í¼ ì‹¤í–‰
    run_scraper(board_url, download_dir)

    print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\n")
        debug_log("í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜", 'ERROR', e)
